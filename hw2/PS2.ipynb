{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem set you will implement a visual transformer and train it on MNIST.\n",
    "The model you'll implement is based on the recent paper https://openreview.net/pdf?id=YicbFdNTTy, and\n",
    "consists of a patch-wise linear projection of images followed by transformer layers that receive the\n",
    "projected patches as a 1-dimensional sequence, akin to how transformers process text data.\n",
    "\n",
    "Reading the paper is not necessary to complete the assignment, and the model you'll implement is a\n",
    "minor simplification of the one proposed by the authors.\n",
    "\n",
    "For reference, the model proposed in the paper is given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between the model above and the one you'll implement are:\n",
    "\n",
    "1 - The sequence will not have an extra token in the beginning.\n",
    "\n",
    "2 - The implemented model will not have patch / position embeddings at all.\n",
    "\n",
    "3 - The classification head will take the whole sequence as input, instead of the first element only.\n",
    "\n",
    "4 - Instead of extracting non-overlapping patches and linearly projecting them, you'll use a convolutional layer (the settings used for the conv layer makes these two equivalent, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We first load the MNIST data like in the previous assignment.\n",
    "# For this homework the images will be cropped to 24x24 pixels, and will not be flattened into vectors.\n",
    "# Each image is a tensor of size 1 x 24 x 24. A batch will be represented as a tensor of size B x 1 x 24 x 24.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import edf\n",
    "import mnist_loader\n",
    "\n",
    "train_images, train_labels = mnist_loader.load_mnist(section = 'training', path = 'MNIST')\n",
    "test_images, test_labels = mnist_loader.load_mnist(section = 'testing', path = 'MNIST')\n",
    "\n",
    "train_images = train_images[:,None,2:-2,2:-2]\n",
    "test_images = test_images[:,None,2:-2,2:-2]\n",
    "\n",
    "plt.imshow(train_images[0,0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are used for training and are the same as the ones in the previous homework.\n",
    "\n",
    "def run_epoch(batch_size, data, labels, xnode, ynode, probnode, lossnode=None):\n",
    "    num_samples = len(data)\n",
    "    total_err = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = num_samples//batch_size\n",
    "    p = np.random.permutation(len(train_images))\n",
    "    shuffled_train_images = train_images[p]\n",
    "    shuffled_train_labels = train_labels[p]\n",
    "    for i in range(num_batches):\n",
    "        start, end = i*batch_size, (i+1)*batch_size\n",
    "        xnode.value = shuffled_train_images[start:end]\n",
    "        ynode.value = shuffled_train_labels[start:end]\n",
    "        edf.Forward()\n",
    "        total_err += np.sum(np.not_equal(np.argmax(probnode.value, axis=1), ynode.value))\n",
    "        if lossnode:\n",
    "            total_loss += lossnode.value.mean()\n",
    "            edf.Backward(lossnode)\n",
    "            edf.SGD()\n",
    "        if i>0 and i%400 == 0:\n",
    "            print (\"\\t Batch {}/{}\".format(i, num_batches))\n",
    "    return 100*total_err/num_samples\n",
    "\n",
    "def train(num_epochs, batch_size, xnode, ynode, probnode, lossnode):\n",
    "    train_err_log = []\n",
    "    test_err_log = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
    "        train_err = run_epoch(batch_size, train_images, train_labels, xnode, ynode, probnode, lossnode)\n",
    "        train_err_log.append(train_err)\n",
    "        print (\"\\t Training Error {:.2f} %\".format(train_err))\n",
    "        test_err = run_epoch(batch_size, test_images, test_labels, xnode, ynode, probnode)\n",
    "        test_err_log.append(test_err)\n",
    "        print (\"\\t Test Error {:.2f} %\".format(test_err))\n",
    "    return train_err_log, test_err_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have new EDF modules that will be helpful when implementing the transformer layers.\n",
    "# Go over them to see how they're used and what they do, since you'll be using them later on.\n",
    "# Note that Affine and Softmax were already given in the previous assignment, but differ from the ones\n",
    "# presented below, so go over them as well.\n",
    "\n",
    "class Affine(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Applies an affine transform to a CompNode x.\n",
    "    \n",
    "    This implementation differs from the one in the previous homework in two ways:\n",
    "    1 - It creates the parameter matrix automatically, so no need to define a AffineParams beforehand.\n",
    "    2 - The input x doesn't have to be a 2nd order tensor (matrix). If x is of higher order, it will be\n",
    "    interpreted as a batch of matrices of dimensions given by the last two shapes.\n",
    "    \n",
    "    For example, if x has shape (64, 9, 10) and you set in_feats = 10, out_feats = 32, which defines a\n",
    "    parameter matrix of size (10, 32), then the output will have shape (64, 9, 32) and will be computed by\n",
    "    multiplying 64 many (9 x 10) matrices with the (10 x 32) parameter matrix, resulting in 64 many (9 x 32)\n",
    "    matrices which are represented as a (64 x 9 x 32) tensor.\n",
    "    \n",
    "    In general, if x has shape (S_1, S_2, ..., S_{d-1}, S_d) and in_feats = S_d, out_feats = S_{d'}, then\n",
    "    the output will have shape (S_1, S_2, ..., S_{d-1}, S_{d'}) and will be computed by performing\n",
    "    a batched matmul over (S_1 * S_2 * ... * S_{d-2}) matrices of size (S_{d-1}, S_d) with the parameter matrix\n",
    "    of size (S_d, S_{d'}). \n",
    "    \"\"\"\n",
    "    def __init__(self, x, in_feats, out_feats):\n",
    "        edf.CompNodes.append(self)\n",
    "        X = edf.Xavier(in_feats)\n",
    "        self.w = edf.Parameter(np.random.uniform(-X,X,(in_feats,out_feats)))\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value @ self.w.value\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad @ self.w.value.transpose())\n",
    "        dw = (np.expand_dims(self.x.value, -1) * np.expand_dims(self.grad, -2)).reshape(-1, *self.w.value.shape)\n",
    "        self.w.addgrad(dw)\n",
    "        \n",
    "class ReLU(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Applies the ReLU activation to a CompNode x, element-wise.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = np.maximum(0, self.x.value)\n",
    "\n",
    "    def backward(self):\n",
    "        self.x.addgrad(np.greater(self.x.value, 0) * self.grad)\n",
    "\n",
    "class Sum(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Performs element-wise addition given two CompNodes x1 and x2.\n",
    "    \n",
    "    Note that x1 and x2 should have matching shapes. \n",
    "    \"\"\"\n",
    "    def __init__(self, x1, x2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x1.value + self.x2.value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x1.addgrad(self.grad)\n",
    "        self.x2.addgrad(self.grad)\n",
    "        \n",
    "class Softmax(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Applies the softmax function over the last dimension of a CompNode s.\n",
    "    \n",
    "    This implementation will accept CompNodes of arbitrary order instead of only matrices.\n",
    "    \"\"\"\n",
    "    def __init__(self, s):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.s = s\n",
    "        \n",
    "    def forward(self):\n",
    "        smax = np.max(self.s.value,axis=-1,keepdims=True)\n",
    "        bounded = np.maximum(-10,self.s.value - smax)\n",
    "        es = np.exp(bounded) \n",
    "        self.value = es / np.sum(es,axis=-1,keepdims=True)\n",
    "\n",
    "    def backward(self):\n",
    "        p_dot_pgrad = np.matmul(np.expand_dims(self.value, -2), np.expand_dims(self.grad, -1)).squeeze(-1)\n",
    "        self.s.addgrad(self.value * (self.grad - p_dot_pgrad))\n",
    "\n",
    "class Reshape(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Reshapes a tensor given the desired output shape.\n",
    "    \n",
    "    Takes as input a CompNode and the desired size, which should be a tuple of ints (excluding batch size!).\n",
    "    For example, if x has shape (64, 1, 24, 24) and size = (576,), then the output will have a shape\n",
    "    of (64, 576). You'll get an error if the shapes have a different number of elements.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, size):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value.reshape(self.x.value.shape[0], *self.size)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad.reshape(self.x.value.shape))\n",
    "\n",
    "class Transpose(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Transposes a tensor given two dimensions.\n",
    "    \n",
    "    Takes as input a CompNode x and two dimensions dim1, dim2, which should be ints, and permutes the\n",
    "    dimensions dim1, dim2 of x.\n",
    "    \n",
    "    For example, if x has shape (64, 32, 9) and dim1 = 1, dim2 = 2, then the output will have a shape\n",
    "    of (64, 9, 32), and will be given by output[i,j,k] = x[i,k,j].\n",
    "    \"\"\"\n",
    "    def __init__(self, x, dim1, dim2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.dim1, self.dim2 = dim1, dim2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x.value.swapaxes(self.dim1, self.dim2)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad.swapaxes(self.dim1, self.dim2))\n",
    "        \n",
    "class BatchedMatmul(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Performs a batched matrix multiplication between two CompNodes x1, x2 of order at most 3.\n",
    "    \n",
    "    Given CompNodex x1, x2 with shapes (S_1, S_2, S_3), (S_1, S_3, S_4), this will compute\n",
    "    S_1 independent matmuls between matrices of shape (S_2, S_3) and (S_3, S_4), each one resulting\n",
    "    in a matrix of shape (S_2, S_4), resulting in an output tensor of shape (S_1, S_2, S_4).\n",
    "    \"\"\"\n",
    "    def __init__(self, x1, x2):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.x1.value @ self.x2.value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.x1.addgrad(self.grad @ self.x2.value.swapaxes(-1,-2))\n",
    "        self.x2.addgrad(self.x1.value.swapaxes(-1,-2) @ self.grad)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you'll have to implement a convolutional layer as an EDF CompNode, including its forward and backward passes.\n",
    "\n",
    "Note that the filter tensor has shape (C_out, C_in, k, k), where C_in is the number of channels that the input has, C_out is how many channels the output will have, and k is the kernel size (we will be considering only square kernels for simplicity).\n",
    "\n",
    "The input is assumed to have shape (B, C_in, S, S), where B is the batch size and S is the spatial resolution -- we'll also be assuming inputs are square images for simplicity.\n",
    "\n",
    "Note that your implementation should also account for padding and stride, which are explained in the course slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Conv(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Applies a convolution to a CompNode x, and takes as parameters in_channels, out_channels, k, padding,\n",
    "    and stride, which should all be ints (k is the kernel size).\n",
    "    \n",
    "    This CompNode will automatically create a EDF Parameter with shape (out_channels, in_channels, k, k)\n",
    "    which will represent the filter bank of the conv layer (ie out_channels many \n",
    "    filters of size in_channels x k x k).\n",
    "    \"\"\"\n",
    "    def __init__(self, x, in_channels, out_channels, k, padding, stride):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.in_channels, self.out_channels, self.k, self.padding, self.stride = in_channels, out_channels, k, padding, stride\n",
    "        gain = edf.Xavier(self.in_channels * self.k**2)\n",
    "        self.w = edf.Parameter(np.random.uniform(-gain,gain,(out_channels, in_channels, k, k)))\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # implementation goes here\n",
    "        batch, in_channels, width, height = self.x.value.shape\n",
    "        padded_width = width + 2 * self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        out_width = (padded_width - self.k) // self.stride + 1\n",
    "        out_height = (padded_height - self.k) // self.stride + 1\n",
    "        \n",
    "        self.padded_x = np.zeros((batch, in_channels, padded_width, padded_height))\n",
    "        self.padded_x[:, :, \n",
    "               self.padding : width + self.padding, \n",
    "               self.padding : height + self.padding] = self.x.value\n",
    "        \n",
    "        self.value = np.empty((batch, self.out_channels, out_width, out_height))\n",
    "        for row in range(out_width):\n",
    "            row_start = row * self.stride\n",
    "            row_end = row_start + self.k\n",
    "            for col in range(out_height):\n",
    "                col_start = col * self.stride\n",
    "                col_end = col_start + self.k\n",
    "                \n",
    "                block = self.padded_x[:, :, row_start : row_end, \n",
    "                                      col_start : col_end]\n",
    "                val = np.tensordot(block, self.w.value,\n",
    "                                   axes=((1, 2, 3), (1, 2, 3)))\n",
    "                \n",
    "                self.value[:, :, row, col] = val\n",
    "        \n",
    "    def backward(self):\n",
    "        # implementation goes here\n",
    "        _, _, width, height = self.x.value.shape\n",
    "        _, _, out_width, out_height = self.value.shape\n",
    "        \n",
    "        # fill in dw and dpadded_x\n",
    "        dw = np.zeros(self.w.value.shape)\n",
    "        dpadded_x = np.zeros(self.padded_x.shape)\n",
    "        for row in range(out_width):\n",
    "            row_start = row * self.stride\n",
    "            row_end = row_start + self.k\n",
    "            for col in range(out_height):\n",
    "                col_start = col * self.stride\n",
    "                col_end = col_start + self.k\n",
    "                grad = self.grad[:, :, row, col]\n",
    "                \n",
    "                temp = np.tensordot(grad, self.w.value, axes=((1), (0)))\n",
    "                dpadded_x[:, :, row_start : row_end, \n",
    "                          col_start : col_end] += temp\n",
    "\n",
    "                temp = self.padded_x[:, :, row_start : row_end, \n",
    "                                      col_start : col_end]\n",
    "                dw += np.tensordot(grad, temp, axes=(0, 0))\n",
    "        \n",
    "        dx = dpadded_x[:, :, self.padding : width + self.padding, \n",
    "                       self.padding : height + self.padding]\n",
    "        self.x.addgrad(dx)\n",
    "        self.w.addgrad(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a small CNN to test your implementation. The cell below will define and train a CNN with 4 conv layers on MNIST, the first three layers having a kernel size of 3, padding of 1 and stride of 2 -- these settings should result in each layer halving the spatial resolution of its input, so the spatial resolution of the first three conv layer outputs should be 12x12, 6x6, and 3x3, respectively. The last layer has a kernel size of 3x3 and no padding, hence its output should have a spatial resolution of 1x1 (and 10 channels). The reshape layer will transform the (B x 10 x 1 x 1) tensor into a (B x 10) one, which will be interpreted as the scores for class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADTVJREFUeJzt3V+MVPUZxvHnLZYLKYqbpiuhWgohGDR22yAaS6rEUrHR4KoxJbEhkbi9YBNNDAnhpnqBIVFpSzTNbiOKSUtt0j+gaSIGVNrYELeIlWKpxtCUzQppcGXFP2TZtxdzaFbK7u8wc+acGd7vJyE7c/bZOa8Tn5z585sz5u4CEM8Xqh4AQDUoPxAU5QeCovxAUJQfCIryA0FRfiAoyg8ERfmBoC4oc2dmxnJCoMnc3fLkGjrym9kyMztoZu+a2dpGbgtAuazetf1mNkXSPyUtlXRY0uuSVrj7gUn+hiM/0GRlHPkXSXrX3d9z95OSfi1peQO3B6BEjZR/lqR/j7t+ONsGoA00/QU/M+uR1NPs/QA4N42Uf1DSZeOufzXb9jnu3i+pX+I5P9BKGnnY/7qkeWb2dTObKukHkrYXMxaAZqv7yO/uo2bWK+lFSVMkbXb3vxc2GYCmqvutvrp2xsN+oOlKWeQDoH1RfiAoyg8ERfmBoCg/EBTlB4Ki/EBQlB8IivIDQVF+ICjKDwRF+YGgKD8QFOUHgqL8QFCUHwiK8gNBlfp1XSjPlClTkpmLL764hElqent7k5kLL7wwmZk/f34ys3r16mTmscceS2YkacWKFcnMp59+msxs2LAhmXn44YdzzVQUjvxAUJQfCIryA0FRfiAoyg8ERfmBoCg/EBTlB4JikU8BLr/88ly5qVOnJjPXX399MrN48eJkZsaMGcnMnXfemcy0msOHDyczmzZtSma6u7tz7W9kZCSZefPNN5OZV199Ndf+ysSRHwiK8gNBUX4gKMoPBEX5gaAoPxAU5QeCovxAUObu5e3MrLydFaSrqyuZ2bVrV67bKvPMOe1obGwsmbn33nuTmY8++qiIcSRJQ0NDycwHH3yQzBw8eLCIcXJxd8uT48gPBNXQ8l4zOyRpRNIpSaPuvrCIoQA0XxFr+5e4+38KuB0AJeJhPxBUo+V3STvM7K9m1nO2gJn1mNmAmQ00uC8ABWr0Yf9idx80s69IesnM/uHuu8cH3L1fUr/Unq/2A+erho787j6Y/Twq6feSFhUxFIDmq7v8ZjbNzKafvizpe5L2FzUYgOaqe5GPmc1R7Wgv1Z4+/Mrd1yf+pu0e9nd0dCQze/bsyXVbc+bMaXSc0uX5bxseHk5mlixZksycPHkymWGhVFreRT51P+d39/ckfaPevwdQLd7qA4Ki/EBQlB8IivIDQVF+ICjKDwRF+YGgKD8QFN/Vl3Ds2LFkZs2aNblu69Zbb01m3njjjWQmz3fR5bFv375kZunSpcnMiRMnkpkrr7wymbn//vuTGRSHIz8QFOUHgqL8QFCUHwiK8gNBUX4gKMoPBEX5gaD4rr4SXXTRRcnMyMhIMtPX15fMrFq1Kpm55557kpmtW7cmM2gtfFcfgElRfiAoyg8ERfmBoCg/EBTlB4Ki/EBQlB8IijP5lOj48eOF3M6HH35YyO3cd999ycxzzz2XzIyNjRUxDkrGkR8IivIDQVF+ICjKDwRF+YGgKD8QFOUHgqL8QFCcyacNTZs2LZl5/vnnk5kbbrghmbnllluSmR07diQzKE9hZ/Ixs81mdtTM9o/b1mFmL5nZO9nPSxoZFkD58jzsf0bSsjO2rZW0093nSdqZXQfQRpLld/fdks78qtrlkrZkl7dIur3guQA0Wb0f7Ol096Hs8vuSOicKmlmPpJ469wOgSRr+VJ+7+2Qv5Ll7v6R+iRf8gFZS71t9R8xspiRlP48WNxKAMtRb/u2SVmaXV0raVsw4AMqS562+rZL+Imm+mR02s1WSNkhaambvSPpudh1AG2GRz3lq7ty5yczevXuTmeHh4WTm5ZdfTmYGBgaSmSeffDKZKfP/13bF13UBmBTlB4Ki/EBQlB8IivIDQVF+ICjKDwRF+YGgWOQTWHd3dzLz9NNPJzPTp08vYhytW7cumXn22WeTmaGhoWTmfMYiHwCTovxAUJQfCIryA0FRfiAoyg8ERfmBoCg/EBSLfDCpq666KpnZuHFjMnPTTTcVMY76+vqSmfXr1yczg4ODRYzTkljkA2BSlB8IivIDQVF+ICjKDwRF+YGgKD8QFOUHgmKRDxo2Y8aMZOa2225LZvKcNcgsvX5l165dyczSpUuTmXbFIh8Ak6L8QFCUHwiK8gNBUX4gKMoPBEX5gaAoPxAUi3zQMj777LNk5oILLkhmRkdHk5mbb74510yvvPJKrlwrKWyRj5ltNrOjZrZ/3LaHzGzQzPZl/77fyLAAypfnYf8zkpadZftP3L0r+/fHYscC0GzJ8rv7bknHSpgFQIkaecGv18z+lj0tuKSwiQCUot7y/1zSXEldkoYkPT5R0Mx6zGzAzAbq3BeAJqir/O5+xN1PufuYpF9IWjRJtt/dF7r7wnqHBFC8uspvZjPHXe2WtH+iLIDWlHzT1My2SrpR0pfN7LCkH0u60cy6JLmkQ5J+1MQZATRBsvzuvuIsm59qwixoQVdffXUyc9dddyUz11xzTTKTZwFPHgcOHEhmdu/eXci+2hnLe4GgKD8QFOUHgqL8QFCUHwiK8gNBUX4gKMoPBFXMqgq0nPnz5yczvb29ycwdd9yRzFx66aW5ZirCqVOnkpmhoaFkZmxsrIhx2hpHfiAoyg8ERfmBoCg/EBTlB4Ki/EBQlB8IivIDQbHIp8XkWTCzYsXZTq70eXkW8MyePTvPSKUZGEif4Hn9+vXJzPbt24sY57zHkR8IivIDQVF+ICjKDwRF+YGgKD8QFOUHgqL8QFCUHwiKFX4F6OzszJVbsGBBMvPEE08kM1dccUWu/ZVlz549ycyjjz6azGzbti2Z4fRbxeHIDwRF+YGgKD8QFOUHgqL8QFCUHwiK8gNBUX4gqOQiHzO7TNKzkjoluaR+d/+ZmXVIek7SbEmHJN3t7h80b9TidXR0JDN9fX3JTFdXV679zZkzJ1euLK+99loy8/jjjyczL774YjLzySef5JoJ5clz5B+V9KC7L5B0naTVZrZA0lpJO919nqSd2XUAbSJZfncfcve92eURSW9LmiVpuaQtWWyLpNubNSSA4p3Tc34zmy3pm5L2SOp099Pfhfy+ak8LALSJ3B/sMbMvSfqtpAfc/biZ/e937u5m5hP8XY+knkYHBVCsXEd+M/uiasX/pbv/Ltt8xMxmZr+fKeno2f7W3fvdfaG7LyxiYADFSJbfaof4pyS97e4bx/1qu6SV2eWVktKfxwTQMvI87P+2pB9KesvM9mXb1knaIOk3ZrZK0r8k3d2cEQE0Q7L87v5nSTbBr28qdhwAZWnLM/lce+21ycyaNWuSmUWLFiUzs2bNyjVTmT7++ONkZtOmTcnMI488ksycOHEi10xoPyzvBYKi/EBQlB8IivIDQVF+ICjKDwRF+YGgKD8QVFsu8unu7i4kU5QDBw7kyr3wwgvJzOjoaDKT5+w6w8PDuWZCXBz5gaAoPxAU5QeCovxAUJQfCIryA0FRfiAoyg8EZe5nPeN2c3Y2wem9ARTH3Sc67d7ncOQHgqL8QFCUHwiK8gNBUX4gKMoPBEX5gaAoPxBU2Wfy+Y9qX+p52pezbe2mHedm5vJUOffX8gZLXeH3fzs3G3D3hZUNUKd2nJuZy9Muc/OwHwiK8gNBVV3+/or3X692nJuZy9MWc1f6nB9Adao+8gOoSGXlN7NlZnbQzN41s7VVzXEuzOyQmb1lZvvMbKDqeSZiZpvN7KiZ7R+3rcPMXjKzd7Kfl1Q545kmmPkhMxvM7u99Zvb9Kmc8k5ldZmYvm9kBM/u7md2fbW/p+/q0SspvZlMkPSnpFkkLJK0wswVVzFKHJe7e1eJv5TwjadkZ29ZK2unu8yTtzK63kmf0/zNL0k+y+7vL3f9Y8kwpo5IedPcFkq6TtDr7/7jV72tJ1R35F0l6193fc/eTkn4taXlFs5x33H23pGNnbF4uaUt2eYuk20sdKmGCmVuauw+5+97s8oiktyXNUovf16dVVf5Zkv497vrhbFurc0k7zOyvZtZT9TDnqNPdh7LL70vqrHKYc9BrZn/Lnha05MNnSTKz2ZK+KWmP2uS+5gW/c7PY3b+l2tOV1Wb2naoHqofX3uJph7d5fi5prqQuSUOS0t9QWgEz+5Kk30p6wN2Pj/9dK9/XVZV/UNJl465/NdvW0tx9MPt5VNLvVXv60i6OmNlMScp+Hq14niR3P+Lup9x9TNIv1IL3t5l9UbXi/9Ldf5dtbov7uqryvy5pnpl93cymSvqBpO0VzZKLmU0zs+mnL0v6nqT9k/9VS9kuaWV2eaWkbRXOksvpAmW61WL3t5mZpKckve3uG8f9qi3u68oW+WRv2/xU0hRJm919fSWD5GRmc1Q72ku1T0P+qlVnNrOtkm5U7dNlRyT9WNIfJP1G0uWqfbLybndvmRfYJpj5RtUe8rukQ5J+NO65dOXMbLGkP0l6S9JYtnmdas/7W/a+Po0VfkBQvOAHBEX5gaAoPxAU5QeCovxAUJQfCIryA0FRfiCo/wKT+WD6jg3GZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# debug forward\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "\n",
    "out = Conv(xnode, 1, 1, 3, 1, 1)\n",
    "\n",
    "out.w.value *= 0\n",
    "out.w.value[0,0,1,1] = 1\n",
    "\n",
    "xnode.value = train_images[:5]\n",
    "edf.Forward()\n",
    "\n",
    "plt.imshow(train_images[0,0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()\n",
    "plt.imshow(out.value[0,0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()\n",
    "\n",
    "print(((train_images[0,0] - out.value[0,0])**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05471545]\n",
      "[0.01852792]\n",
      "[0.00968743]\n",
      "[0.00796954]\n",
      "[0.00675685]\n",
      "[0.00590582]\n",
      "[0.00528648]\n",
      "[0.0048311]\n",
      "[0.00448884]\n",
      "[0.0042274]\n",
      "[0.00402325]\n",
      "[0.00386056]\n",
      "[0.0037279]\n",
      "[0.00361737]\n",
      "[0.00352326]\n",
      "[0.00344159]\n",
      "[0.00336945]\n",
      "[0.00330476]\n",
      "[0.00324602]\n",
      "[0.00319211]\n",
      "[0.00314221]\n",
      "[0.0030957]\n",
      "[0.0030521]\n",
      "[0.00301106]\n",
      "[0.00297226]\n",
      "[0.00293549]\n",
      "[0.00290054]\n",
      "[0.00286726]\n",
      "[0.0028355]\n",
      "[0.00280514]\n",
      "[0.00277609]\n",
      "[0.00274824]\n",
      "[0.00272152]\n",
      "[0.00269585]\n",
      "[0.00267116]\n",
      "[0.00264739]\n",
      "[0.00262448]\n",
      "[0.00260238]\n",
      "[0.00258104]\n",
      "[0.00256043]\n",
      "[0.00254049]\n",
      "[0.00252118]\n",
      "[0.00250249]\n",
      "[0.00248436]\n",
      "[0.00246677]\n",
      "[0.00244969]\n",
      "[0.00243309]\n",
      "[0.00241696]\n",
      "[0.00240125]\n",
      "[0.00238596]\n",
      "[0.00237107]\n",
      "[0.00235655]\n",
      "[0.00234238]\n",
      "[0.00232855]\n",
      "[0.00231504]\n",
      "[0.00230184]\n",
      "[0.00228894]\n",
      "[0.00227631]\n",
      "[0.00226395]\n",
      "[0.00225184]\n",
      "[0.00223998]\n",
      "[0.00222835]\n",
      "[0.00221694]\n",
      "[0.00220575]\n",
      "[0.00219476]\n",
      "[0.00218396]\n",
      "[0.00217336]\n",
      "[0.00216292]\n",
      "[0.00215266]\n",
      "[0.00214257]\n",
      "[0.00213263]\n",
      "[0.00212284]\n",
      "[0.00211319]\n",
      "[0.00210368]\n",
      "[0.0020943]\n",
      "[0.00208505]\n",
      "[0.00207592]\n",
      "[0.0020669]\n",
      "[0.002058]\n",
      "[0.00204919]\n",
      "[0.00204049]\n",
      "[0.00203188]\n",
      "[0.00202337]\n",
      "[0.00201494]\n",
      "[0.00200659]\n",
      "[0.00199833]\n",
      "[0.00199013]\n",
      "[0.00198201]\n",
      "[0.00197396]\n",
      "[0.00196597]\n",
      "[0.00195804]\n",
      "[0.00195016]\n",
      "[0.00194234]\n",
      "[0.00193457]\n",
      "[0.00192685]\n",
      "[0.00191917]\n",
      "[0.00191153]\n",
      "[0.00190393]\n",
      "[0.00189637]\n",
      "[0.00188884]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# debug backward\n",
    "class L2Loss(edf.CompNode):\n",
    "    def __init__(self, x, t):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "    def forward(self):\n",
    "        self.value = (0.5 * (self.x.value - self.t.value)**2).mean((1,2,3))\n",
    "    def backward(self):\n",
    "        self.x.addgrad(self.grad * (self.x.value - self.t.value) / np.prod(self.x.value.shape[1:]))\n",
    "\n",
    "np.random.seed(1234)\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "out = Conv(xnode, 1, 1, 3, 1, 1)\n",
    "out = Conv(out, 1, 1, 3, 1, 1)\n",
    "lossnode = L2Loss(out, ynode)\n",
    "edf.learning_rate = 1\n",
    "xnode.value = train_images[:1]\n",
    "ynode.value = train_images[:1]\n",
    "\n",
    "for i in range(100):\n",
    "    edf.Forward()\n",
    "    edf.Backward(lossnode)\n",
    "    edf.SGD()\n",
    "    print(lossnode.value)\n",
    "#     print(out.w.value)\n",
    "plt.imshow(out.value[0,0], cmap='gray', interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 81.77 %\n",
      "\t Test Error 81.38 %\n",
      "Epoch: 2/5\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 74.61 %\n",
      "\t Test Error 73.37 %\n",
      "Epoch: 3/5\n",
      "\t Batch 400/937\n",
      "\t Batch 800/937\n",
      "\t Training Error 72.29 %\n",
      "\t Test Error 70.17 %\n",
      "Epoch: 4/5\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "batch_size = 64\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "\n",
    "def CNN(x, y):\n",
    "    u = ReLU(Conv(x, 1, 4, 3, 1, 2))\n",
    "    u = ReLU(Conv(u, 4, 8, 3, 1, 2))\n",
    "    u = ReLU(Conv(u, 8, 16, 3, 1, 2))\n",
    "    u = Conv(u, 16, 10, 3, 0, 1)\n",
    "    u = Reshape(u, (10,))\n",
    "    probnode = edf.Softmax(u)\n",
    "    lossnode = edf.LogLoss(probnode, ynode)\n",
    "    return probnode, lossnode\n",
    "\n",
    "probnode, lossnode = CNN(xnode, ynode)\n",
    "\n",
    "num_epochs = 5\n",
    "edf.learning_rate = 0.5\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented correctly, the model above should be able to achieve less than 3% train/test error.\n",
    "\n",
    "You'll also want the implementation to be somewhat optimized such that the above cell doesn't take more than ~10 minutes to run. To do that, you should have no more than 2 nested loops and all remaining dimension-wise operations should be properly vectorized via numpy calls.\n",
    "\n",
    "Ideally your implementation should have no Python loops at all and rely purely on numpy calls and smart vectorization. As a hint to achieve that, note that when convolving a C x k x k kernel over a C x S x S image, each element in the output tensor will be given by a inner product between the kernel and a C x k x k slice of the input image, therefore if the output has shape S' x S', then it can be computed directly by a inner product between a S' x S' x C x k x k tensor (which stores S'^2 many input slices of shape C x k x k) and the C x k x k filter. Note that this holds for each filter of the conv layer independently, so you'll need to account for an extra dimension when computing the output of all filters at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"error (%)\")\n",
    "plt.plot(np.arange(len(test_err_log)), test_err_log, color='red')\n",
    "plt.plot(np.arange(len(train_err_log)), train_err_log, color='blue')\n",
    "plt.legend(['test error', 'train error'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have to implement a LayerNorm and a SelfAttention layer in EDF.\n",
    "\n",
    "For LayerNorm, you don't need to implement the learnable affine transform that follows normalization, so the layer shouldn't have parameters at all. We will only apply it on input tensors of 3rd order, so your implementation doesn't have to work for arbitrarily-sized input tensors.\n",
    "\n",
    "Note that SelfAttention below is --not-- a CompNode class. It turns out that SelfAttention can be implemented by simply stacking layers that have already been defined, so you don't have to worry about implementing a forward and backward pass for it. However, if you can't figure out how to implement it by stacking the given components, you can implement a CompNode instead as long as your forward and backward passes are correct.\n",
    "\n",
    "Also note that you only need to implement single-head attention. This should be considerably easier than multi-head since input, intermediate, and output tensors will be always of 3rd order instead of 4th, hence some layers defined above can be used (but wouldn't if you were to implement multi-head attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(edf.CompNode):\n",
    "    \"\"\"\n",
    "    Applies Layer Normalization to an input tensor of 3rd order.\n",
    "    \n",
    "    More specifically, given an input tensor of size (S_1, S_2, S_3), Layer Norm computes the mean and\n",
    "    deviation of each of the S_1 many matrices of shape (S_2, S_3) and normalizes each matrix element using\n",
    "    the correspondent mean and std.\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        edf.CompNodes.append(self)\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self):\n",
    "        # implementation goes here\n",
    "        # take mu and variance for each S_2 * S_3 matrix\n",
    "        self.mu = self.x.value.mean(axis=(1, 2), keepdims=True)\n",
    "        self.variance = self.x.value.var(axis=(1, 2), keepdims=True)\n",
    "        self.std = self.x.value.std(axis=(1, 2), keepdims=True)\n",
    "        self.residuals = self.x.value - self.mu\n",
    "        self.value = self.residuals / self.std\n",
    "        \n",
    "    def backward(self):\n",
    "        # implementation goes here\n",
    "        d_mu = np.zeros(self.mu.shape)\n",
    "        d_variance = np.zeros(self.variance.shape)\n",
    "        \n",
    "        temp = self.grad * np.reciprocal(self.std)\n",
    "        self.x.addgrad(temp)\n",
    "        d_mu -= np.sum(temp, axis=(1, 2), keepdims=True)\n",
    "        d_variance -= np.sum(self.grad * self.residuals,\n",
    "                             axis=(1, 2), keepdims=True) * \\\n",
    "        np.reciprocal(self.variance * self.std) / 2\n",
    "        \n",
    "        _, S_2, S_3 = self.x.value.shape\n",
    "        temp = 2 * d_variance * self.residuals / S_2 / S_3\n",
    "        self.x.addgrad(temp)\n",
    "        d_mu -= np.sum(temp, axis=(1, 2), keepdims=True)\n",
    "        \n",
    "        self.x.addgrad(d_mu / S_2 / S_3)\n",
    "        \n",
    "def SelfAttention(x, in_dim, att_dim):\n",
    "    \"\"\"\n",
    "    Applies single-head Self Attention to a input CompNode x. in_dim should be the number of \n",
    "    features of x (a int), and att_dim is the dimension of the query, key, and value tensors.\n",
    "    \n",
    "    The input x is assumed to be a 3rd order tensor with size (B, T, in_dim), where B is the batch size,\n",
    "    T is the sequence length, and in_dim is the number of features. The layer produces query, key, and value\n",
    "    tensors, each of shape (B, T, att_dim) via a linear transformation acting on x.\n",
    "    \n",
    "    Afterwards, the attention tensor of shape (B, T, T) is computed from q and k, as described in the \n",
    "    course material. Normalizing the input of the softmax function by sqrt(att_dim) is optional.\n",
    "    Then, an intermediate tensor h is produced from the attention and value tensors, having \n",
    "    shape (B, T, att_dim). Finally, the output of shape (B, T, in_dim) is generated by applying a \n",
    "    linear transformation on h.\n",
    "    \"\"\"\n",
    "    q = Affine(x, in_dim, att_dim)\n",
    "    k = Affine(x, in_dim, att_dim)\n",
    "    v = Affine(x, in_dim, att_dim)\n",
    "    att = Softmax(BatchedMatmul(q, Transpose(k, 1, 2)))\n",
    "    h = BatchedMatmul(att, v)\n",
    "    out = Affine(h, att_dim, in_dim)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you'll implement a TransformerLayer, which is composed of a residual block with a LayerNorm -> SelfAttention trunk, followed by another residual block with a LayerNorm -> Affine -> ReLU -> Affine trunk, where the first Affine layer doubles the number of features and the second halves it, such that the output has the same shape as the input.\n",
    "\n",
    "More specifically, the layer should be:\n",
    "\n",
    "u = SelfAttention(LayerNorm(x)) + x\n",
    "\n",
    "out = Affine(ReLU(Affine(LayerNorm(x)))) + u\n",
    "\n",
    "Remember that addition in the residual blocks needs to be backprop'able!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerLayer(x, in_dim, att_dim):\n",
    "    # implementation goes here\n",
    "    u = Sum(SelfAttention(LayerNorm(x), in_dim, att_dim), x)\n",
    "    out = Sum(Affine(ReLU(Affine(\n",
    "        LayerNorm(x), in_dim, att_dim)), in_dim, att_dim), x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train a Visual Transformer with two Transformer layers. If your implementation is correct, then it should be able to achieve around 2% train/test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "batch_size = 64\n",
    "edf.clear_compgraph()\n",
    "xnode = edf.Input()\n",
    "ynode = edf.Input()\n",
    "\n",
    "dim = 32\n",
    "patch_size = 8\n",
    "seq_len = (24 // patch_size)**2\n",
    "\n",
    "u = Conv(xnode, 1, dim, patch_size, 0, patch_size)\n",
    "u = Reshape(u, (dim, seq_len))\n",
    "u = Transpose(u, 1, 2)\n",
    "\n",
    "u = TransformerLayer(u, dim, dim)\n",
    "u = TransformerLayer(u, dim, dim)\n",
    "\n",
    "u = Reshape(u, (seq_len*dim,))\n",
    "scores = Affine(u, seq_len*dim, 10)\n",
    "\n",
    "probnode = edf.Softmax(scores)\n",
    "lossnode = edf.LogLoss(probnode, ynode)\n",
    "\n",
    "num_epochs = 10\n",
    "edf.learning_rate = 0.05\n",
    "train_err_log, test_err_log = train(num_epochs, batch_size, xnode, ynode, probnode, lossnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
